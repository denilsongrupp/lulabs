# -*- coding: utf-8 -*-
"""luiza_lab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WZie8NAINqP1_ig9UxFPbjN1aUIfUKO6

## Parte 1 - Desafio bem tranquilo, preferi utilizar um conceito simples e focar na parte de pedidos.
## Entendi que a saida podia ser uma lista com a contagem das palavras em dicionario e mais dois elementos com as informações de maior ou até 10 caracteres
"""

import requests

r = requests.get("https://storage.googleapis.com/luizalabs-hiring-test/wordcount.txt")
txt = r.text
wordfreq = []
ate = 0
mais = 0

for w in txt.split():
    wordfreq.append({w:txt.count(w)})
    if len(w) <= 10:
      ate +=1
    else:
      mais +=1
wordfreq.append({"MAIORES QUE 10":mais})
wordfreq.append({"ATÉ 10":ate})

"""## Grava dados"""

f=open('frequencia.txt','w')
l1=map(lambda x:str(x)+'\n', wordfreq)
f.writelines(l1)
f.close()

"""## Instalando o spark no meu notebook e criando uma sessão

"""

# Commented out IPython magic to ensure Python compatibility.
# %%sh
# sudo pip install spark
# sudo pip install pyspark

import pandas as pd
import spark,pyspark
from pyspark.sql import *
from pyspark.sql import functions as f
from pyspark.sql import types as t
from pyspark.sql.functions import col,when,lit,row_number
from pyspark.sql.window import Window
from datetime import date

config = pyspark.SparkConf().setAll([('spark.executor.memory', '8g'),\
                                     ('spark.executor.cores', '3'),\
                                     ('spark.cores.max', '3'),\
                                     ('spark.driver.memory','8g')                             
                                     ] )
 
#sc = pyspark.SparkContext(conf=config)
import pandas as pd
spark = SparkSession.builder.config(conf=config).getOrCreate()

"""## Criei um arquivo contendo as datas das ultimas blackfridays de 2018,2019 e 2020, solução muito mais maleável, para adicionar futuras datas sem ficar inserindo valores no código. o Arquivo é blackfriday.txt"""

bf = spark.read.format("csv").option("header","true").load("blackfriday.txt")

"""## Leitura do csv e tratamento na tipagem dos campos data_pedido e data_nascimento_cliente_dt"""

df = spark.read.format("csv").option("header","true").load("clientes_pedidos.csv")
#Conventertendo para data
df = df.withColumn("data_pedido",f.from_unixtime('data_pedido').cast(t.DateType()))
df = df.withColumn("data_nascimento_cliente_dt",df.data_nascimento_cliente.cast(t.DateType()))
#Idade
df = df.withColumn("idade",((f.months_between(f.lit(date.today()),df.data_nascimento_cliente_dt)/12).cast(t.IntegerType())))
#Join black friday
df = df.join(bf,(df.data_pedido==bf.data),how='left')
df.show(10)

"""## Cliente com mais de 2 compras na black friday. """

df = df.withColumn("compras_bf",f.sum(df.flg_bf).over(Window.partitionBy("codigo_cliente")))
df_black = df.where(df.compras_bf>2).toPandas()
df_black.to_csv("black_friday.csv")

#df pedidos dos clientes menores que trinta
 df_3 = df.where(df.idade<30)
 df_31 = df_3.groupBy(df_3.codigo_cliente,df_3.idade).agg(
     f.count(df_3.codigo_pedido).alias("numero_pedido"),
     f.collect_list(f.struct(df_3.codigo_pedido,df_3.data_pedido)
 ).alias("lista_pedidos"))
 df_32 = df_31.toPandas()
 df_32.to_csv("menores_30.csv")

df_32.head(10)

